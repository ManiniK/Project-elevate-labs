# -*- coding: utf-8 -*-
"""mental_health_chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oiQvZ_bkYsL6IfnkK5dyShb2UCdgcUx4
"""

!pip install langchain_groq langchain_core langchain-community

from langchain_groq import ChatGroq
llm=ChatGroq(
    temperature = 0,
    groq_api_key = "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",
    model_name = "llama-3.3-70b-versatile"
)
result= llm.invoke("Who is lord Ram? ")
print(result.content)

!pip install pypdf

!pip install chromadb

!pip install sentence_transformers



import os
from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter

def initialize_llm():
  llm=ChatGroq(
    temperature = 0,
    groq_api_key = "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",
    model_name = "llama-3.3-70b-versatile"
)
  return llm


def create_vector_db():
    loader=DirectoryLoader("/content/data", glob="*.pdf",loader_cls=PyPDFLoader)
    documents= loader.load()
    text_splitter=RecursiveCharacterTextSplitter(chunk_size= 500, chunk_overlap=50)
    texts=text_splitter.split_documents(documents)
    embeddings= HuggingFaceBgeEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vector_db=Chroma.from_documents(texts, embeddings, persist_directory="./chroma_db")
    vector_db.persist()
    print("ChromaDB created and data saved")
    return vector_db

def setup_qa_chain(vector_db,llm):
  retriever = vector_db.as_retriever()
  prompt_template = """ You are a compassionate mental health chatbot. Respond thoughtfully to the following question: {context}
  User: {question}
  Chatbot:"""
  PROMPT = PromptTemplate(
      template=prompt_template, input_variables=["context", "question"])

  qa_chain= RetrievalQA.from_chain_type(
      llm=llm,
      chain_type="stuff",
      retriever=retriever,
      chain_type_kwargs={"prompt":PROMPT}
  )
  return qa_chain


def main():
  print("Initialising Chatbot....")
  llm = initialize_llm()
  db_path = "/content/chroma_db"

  if not os.path.exists(db_path):
    vector_db = create_vector_db()
  else:
    embeddings = HuggingFaceBgeEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)
  qa_chain = setup_qa_chain(vector_db, llm)

  while True:
    query = input("\nHuman: ")
    if query.lower() == "exit":
      print("Chatbot: Take care of yourself cutie, bye!")
      break
    response = qa_chain.run(query)
    print(f"Chatbot: {response}")

if __name__ == "__main__":
    main()

create_vector_db()

!pip install gradio

!pip install --upgrade gradio

import os
from langchain_groq import ChatGroq
from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
import gradio as gr

def initialize_llm():
  llm=ChatGroq(
    temperature = 0,
    groq_api_key = "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",
    model_name = "llama-3.3-70b-versatile"
)
  return llm


def create_vector_db():
    loader=DirectoryLoader("/content/data", glob="*.pdf",loader_cls=PyPDFLoader)
    documents= loader.load()
    text_splitter=RecursiveCharacterTextSplitter(chunk_size= 500, chunk_overlap=50)
    texts=text_splitter.split_documents(documents)
    embeddings= HuggingFaceBgeEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vector_db=Chroma.from_documents(texts, embeddings, persist_directory="./chroma_db")
    vector_db.persist()
    print("ChromaDB created and data saved")
    return vector_db

def setup_qa_chain(vector_db,llm):
  retriever = vector_db.as_retriever()
  prompt_template = """ You are a compassionate mental health chatbot. Respond thoughtfully to the following question: {context}
  User: {question}
  Chatbot:"""
  PROMPT = PromptTemplate(
      template=prompt_template, input_variables=["context", "question"])

  qa_chain= RetrievalQA.from_chain_type(
      llm=llm,
      chain_type="stuff",
      retriever=retriever,
      chain_type_kwargs={"prompt":PROMPT}
  )
  return qa_chain


print("Initialising Chatbot....")
llm = initialize_llm()
db_path = "/content/chroma_db"

if not os.path.exists(db_path):
  vector_db = create_vector_db()
else:
  embeddings = HuggingFaceBgeEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
  vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)
qa_chain = setup_qa_chain(vector_db, llm)

def chatbot_response(user_input, history):
  if not user_input.strip():
    return history
  try:
    response = qa_chain.run(user_input)
  except Exception as e:
    response = f"Error: {e}"
  history.append((user_input, response))
  return history



with gr.Blocks(theme='earneleh/paris') as app:
  chatbot = gr.Chatbot()
  user_input = gr.Textbox(placeholder="Type your message here...")
  send_btn = gr.Button("Send")

  def respond(user_message, chat_history):
    chat_history = chatbot_response(user_message, chat_history)
    return chatbot, chat_history

  send_btn.click(respond, inputs=[user_input, chatbot], outputs=[chatbot, chatbot])
app.launch()



